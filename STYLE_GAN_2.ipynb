{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "saHCenBVXkiz",
        "HmsJ3VvLXr8D",
        "vkoIl841ai0A",
        "LUmeuuB7mUXy",
        "sOf7Dr6svZh-",
        "deqm2gJTvctp",
        "iFmf2Br1KiR4",
        "mQgUwambvybi"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "saHCenBVXkiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/models"
      ],
      "metadata": {
        "id": "OPeIyVICdykL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc54a6ce-782c-4e40-8742-e03cc0d7f361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/results"
      ],
      "metadata": {
        "id": "zNv2ILDCd2L6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c755831-e62a-4628-d44e-4098211008fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/sample_data"
      ],
      "metadata": {
        "id": "ofAbV0Zpd9sY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299df712-195d-419a-d395-e373d38799a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops vector-quantize-pytorch\n",
        "!pip install kornia==0.5.4\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from math import floor, log2\n",
        "from random import random\n",
        "from shutil import rmtree\n",
        "from functools import partial\n",
        "import multiprocessing\n",
        "from contextlib import contextmanager, ExitStack\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "from torch.utils import data\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad as torch_grad\n",
        "from einops import rearrange, repeat\n",
        "from kornia.filters import filter2D\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from vector_quantize_pytorch import VectorQuantize\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhN2rFugXhls",
        "outputId": "75081ae5-8ce3-4796-ceb8-87cc64483d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: vector-quantize-pytorch in /usr/local/lib/python3.10/dist-packages (1.11.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vector-quantize-pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->vector-quantize-pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->vector-quantize-pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->vector-quantize-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vector-quantize-pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->vector-quantize-pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->vector-quantize-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->vector-quantize-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->vector-quantize-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->vector-quantize-pytorch) (1.3.0)\n",
            "Requirement already satisfied: kornia==0.5.4 in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kornia==0.5.4) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from kornia==0.5.4) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->kornia==0.5.4) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->kornia==0.5.4) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->kornia==0.5.4) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->kornia==0.5.4) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->kornia==0.5.4) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->kornia==0.5.4) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->kornia==0.5.4) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->kornia==0.5.4) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->kornia==0.5.4) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load DataSet"
      ],
      "metadata": {
        "id": "HmsJ3VvLXr8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = os.listdir(\"/content/drive/MyDrive/face\")\n",
        "image_paths = [f\"/content/drive/MyDrive/face/{el}\" for el in image_paths]"
      ],
      "metadata": {
        "id": "1X3uBXvU996Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35db2c87-a12a-430e-c934-c729c793b157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "        __init__ class intitalizer.\n",
        "        :param p1: self object\n",
        "        :param p2: Contains paths to the images\n",
        "    \"\"\"\n",
        "    def __init__(self, image_paths):\n",
        "        super().__init__()\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((128, 128)), # resizing images to (128, 128)\n",
        "            transforms.RandomGrayscale(0.5), # applying random grayscale with a probability of 0.5\n",
        "            transforms.RandomHorizontalFlip(0.5),  # performing random horizontal flipping with a probability of 0.5\n",
        "            transforms.ToTensor() # converting images to PyTorch tensors\n",
        "        ])\n",
        "        self.cutout = transforms.Compose([ #a sequence of seven RandomErasing transformations\n",
        "            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n",
        "            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n",
        "            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n",
        "            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)), #creates a mask by erasing random rectangles from the image with varying scales and probabilities\n",
        "            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n",
        "            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n",
        "            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n",
        "            transforms.RandomErasing(p=0.66, value=1.0, scale=(0.02, 0.1)),\n",
        "        ])\n",
        "        self.normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # normalizes the images using mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5) for each channel\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "            __len__  to get the length of the dataset.\n",
        "\n",
        "            :param p1: self object\n",
        "            :return: the length of the dataset ie  number of images\n",
        "        \"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "          __getitem__ class intitalizer.\n",
        "\n",
        "            :param p1: an image at a given\n",
        "            :param p1: self object\n",
        "            :return: the normalized versions of the original image\n",
        "        \"\"\"\n",
        "        image = Image.open(self.image_paths[index])\n",
        "        image = self.transform(image) # Applies the defined transformations (self.transform) to the image.\n",
        "        mask = torch.zeros((3, 128, 128)) # Creates an initial mask (initialized as black) with the same size as the image.\n",
        "        mask = self.cutout(mask) # apply random erasing operations to the mask and generates a masked version of the image (image_cutout) using this mask.\n",
        "        image_cutout = torch.where(mask == 1.0, torch.ones_like(image), image)\n",
        "        return self.normalize(image), self.normalize(image_cutout), self.normalize(mask), mask # normalized versions of the original image, the masked image (image_cutout), the normalized mask, and the original mask."
      ],
      "metadata": {
        "id": "ezux-G4H-Kbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f95dec4-c63c-4010-9b71-fadd33a518e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Util Functions"
      ],
      "metadata": {
        "id": "vkoIl841ai0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CORES = multiprocessing.cpu_count() # number of CPU cores available in the system using the multiprocessing.cpu_count()\n",
        "EXTS = ['jpg', 'jpeg', 'png']"
      ],
      "metadata": {
        "id": "gXxZsFG_aq7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419d03f4-68f4-4309-dbf5-605f4a738206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#custom exception classes\n",
        "class NanException(Exception):\n",
        "    pass"
      ],
      "metadata": {
        "id": "vSZHSkT_atOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e855bc-1f91-44f9-826f-997ac572269f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''''\n",
        "This class gives the moving average\n",
        "'''\n",
        "class EMA():\n",
        "    \"\"\"\n",
        "        __init__ class intitalizer.\n",
        "\n",
        "        :param p1: self object\n",
        "        :param p2: smoothing factor applied to the old average when updating it with a new value\n",
        "    \"\"\"\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "    \"\"\"\n",
        "          update_average class intitalizer.\n",
        "          :param p1: self object\n",
        "          :param p2: old moving average\n",
        "          :param p3: new moving average\n",
        "          :return: exponential moving average\n",
        "    \"\"\"\n",
        "    def update_average(self, old, new):\n",
        "        if not exists(old):\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new"
      ],
      "metadata": {
        "id": "aXusSqEjavGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d2bdd5-6a9c-4c04-d759-80d371f678c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''''\n",
        "To flatten the input tensor\n",
        "'''\n",
        "class Flatten(nn.Module):\n",
        "    \"\"\"\n",
        "            to perform the flattening operation on the input tensor x.\n",
        "            :param p1: self object\n",
        "            :param p2: old moving average\n",
        "            :return: a flatten the tensor while maintaining the batch size (x.shape[0]) and reshaping the remaining dimensions into a single dimension by using -1\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return x.reshape(x.shape[0], -1)"
      ],
      "metadata": {
        "id": "Gx-t_wK-a0LO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e4af79-39c2-431b-da5b-213d64d4fa0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomApply(nn.Module):\n",
        "    def __init__(self, prob, fn, fn_else = lambda x: x):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.fn_else = fn_else\n",
        "        self.prob = prob\n",
        "    def forward(self, x):\n",
        "        fn = self.fn if random() < self.prob else self.fn_else\n",
        "        return fn(x)"
      ],
      "metadata": {
        "id": "P1uOe9Iza3I6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9bdcd0f-6807-480a-9656-19725cd216d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x):\n",
        "        return self.fn(x) + x"
      ],
      "metadata": {
        "id": "eOIW4ukNa7GU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19da54e4-dca9-44f4-bf2f-d457cf8b70c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = ChanNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(self.norm(x))"
      ],
      "metadata": {
        "id": "YNke_tLka97X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b59072f-91d0-4c77-f5b0-5551d7e62e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChanNorm(nn.Module):\n",
        "    def __init__(self, dim, eps = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
        "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        std = torch.var(x, dim = 1, unbiased = False, keepdim = True).sqrt()\n",
        "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
        "        return (x - mean) / (std + self.eps) * self.g + self.b"
      ],
      "metadata": {
        "id": "GGJoJLEXbBe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1fd341-9483-4768-d8bd-c8249b8e3892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PermuteToFrom(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        out, loss = self.fn(x)\n",
        "        out = out.permute(0, 3, 1, 2)\n",
        "        return out, loss"
      ],
      "metadata": {
        "id": "klU4LxcBbEHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c71d7c-32b4-4d3e-abed-b68758a43567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Blur(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        f = torch.Tensor([1, 2, 1])\n",
        "        self.register_buffer('f', f)\n",
        "    def forward(self, x):\n",
        "        f = self.f\n",
        "        f = f[None, None, :] * f [None, :, None]\n",
        "        return filter2D(x, f, normalized=True)"
      ],
      "metadata": {
        "id": "KONZFtt5bGXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a773033-b4ce-4f7f-894d-6780ef844329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attention\n",
        "class DepthWiseConv2d(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, kernel_size, padding = 0, stride = 1, bias = True):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias = bias),\n",
        "            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "aNfOqcXXbKb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d18bf2d-399c-4062-d96c-ce4c026c0561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, dim_head = 64, heads = 8):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.nonlin = nn.GELU()\n",
        "        self.to_q = nn.Conv2d(dim, inner_dim, 1, bias = False)\n",
        "        self.to_kv = DepthWiseConv2d(dim, inner_dim * 2, 3, padding = 1, bias = False)\n",
        "        self.to_out = nn.Conv2d(inner_dim, dim, 1)\n",
        "\n",
        "    def forward(self, fmap):\n",
        "        h, x, y = self.heads, *fmap.shape[-2:]\n",
        "        q, k, v = (self.to_q(fmap), *self.to_kv(fmap).chunk(2, dim = 1))\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = h), (q, k, v))\n",
        "\n",
        "        q = q.softmax(dim = -1)\n",
        "        k = k.softmax(dim = -2)\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        context = einsum('b n d, b n e -> b d e', k, v)\n",
        "        out = einsum('b n d, b d e -> b n e', q, context)\n",
        "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n",
        "\n",
        "        out = self.nonlin(out)\n",
        "        return self.to_out(out)"
      ],
      "metadata": {
        "id": "IAYGfO-1bRsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6718430-926e-49df-9bfb-e9027bff8455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one layer of self-attention and feedforward, for images\n",
        "attn_and_ff = lambda chan: nn.Sequential(*[\n",
        "    Residual(PreNorm(chan, LinearAttention(chan))),\n",
        "    Residual(PreNorm(chan, nn.Sequential(nn.Conv2d(chan, chan * 2, 1), leaky_relu(), nn.Conv2d(chan * 2, chan, 1))))\n",
        "])"
      ],
      "metadata": {
        "id": "V8d16p3FbU7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a2e54f-f11e-4ff5-af1e-b8ec828dfd97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helpers\n",
        "def exists(val):\n",
        "    return val is not None"
      ],
      "metadata": {
        "id": "dTYRoaMobX1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbfa00b3-c3ad-4413-857f-f5b0e351f4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@contextmanager\n",
        "def null_context():\n",
        "    yield"
      ],
      "metadata": {
        "id": "d48SyjOvbZ4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d1aaea-4009-45d6-fc0a-740d0a765217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_contexts(contexts):\n",
        "    @contextmanager\n",
        "    def multi_contexts():\n",
        "        with ExitStack() as stack:\n",
        "            yield [stack.enter_context(ctx()) for ctx in contexts]\n",
        "    return multi_contexts"
      ],
      "metadata": {
        "id": "RY2cxB8jbhyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2310c99-1227-4f33-f526-31184d8305f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def default(value, d):\n",
        "    return value if exists(value) else d"
      ],
      "metadata": {
        "id": "vUkj2mH5bkBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9313815-a19b-4b61-8451-151f7b5872ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for i in iterable:\n",
        "            yield i"
      ],
      "metadata": {
        "id": "eF0BzVhObm9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d92648-e072-43d8-aad6-47a4eb2f3af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cast_list(el):\n",
        "    return el if isinstance(el, list) else [el]"
      ],
      "metadata": {
        "id": "AE4JaWVVbo6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5f72e6-4f09-4959-df77-5c65adc9f76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_empty(t):\n",
        "    if isinstance(t, torch.Tensor):\n",
        "        return t.nelement() == 0\n",
        "    return not exists(t)"
      ],
      "metadata": {
        "id": "XnqsxNYvbrSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8188c43a-b691-4463-82a3-557670aa7d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def raise_if_nan(t):\n",
        "    if torch.isnan(t):\n",
        "        raise NanException"
      ],
      "metadata": {
        "id": "cHfRQukMbtmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d5c58d8-2404-49d4-b984-46e9a6d74a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(images, output, weight = 10):\n",
        "    batch_size = images.shape[0]\n",
        "    gradients = torch_grad(outputs=output, inputs=images,\n",
        "                           grad_outputs=torch.ones(output.size(), device=images.device),\n",
        "                           create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradients = gradients.reshape(batch_size, -1)\n",
        "    return weight * ((gradients.norm(2, dim=1) - 1) ** 2).mean()"
      ],
      "metadata": {
        "id": "uf5wAaaybwWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ceb9da0-f9b4-47b0-8a9c-76b260f2fd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_pl_lengths(styles, images):\n",
        "    device = images.device\n",
        "    num_pixels = images.shape[2] * images.shape[3]\n",
        "    pl_noise = torch.randn(images.shape, device=device) / math.sqrt(num_pixels)\n",
        "    outputs = (images * pl_noise).sum()\n",
        "\n",
        "    pl_grads = torch_grad(outputs=outputs, inputs=styles,\n",
        "                          grad_outputs=torch.ones(outputs.shape, device=device),\n",
        "                          create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    return (pl_grads ** 2).sum(dim=2).mean(dim=1).sqrt()"
      ],
      "metadata": {
        "id": "imcAMq58bzdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efeb6e25-2033-4356-85eb-76c7b8cff570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def noise(n, latent_dim, device):\n",
        "    return torch.randn(n, latent_dim).cuda(device)"
      ],
      "metadata": {
        "id": "a0Hu0S8mb2YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea339884-ae98-4222-8fa4-de0874d4f37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def noise_list(n, layers, latent_dim, device):\n",
        "    return [(noise(n, latent_dim, device), layers)]"
      ],
      "metadata": {
        "id": "1znKCtVeb4yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad245bf1-2634-475b-bf63-2d79ff4b8ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mixed_list(n, layers, latent_dim, device):\n",
        "    tt = int(torch.rand(()).numpy() * layers)\n",
        "    return noise_list(n, tt, latent_dim, device) + noise_list(n, layers - tt, latent_dim, device)"
      ],
      "metadata": {
        "id": "VqN7FJVIb7bY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117fe514-0564-465f-e053-8d86487173f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def latent_to_w(style_vectorizer, latent_descr):\n",
        "    \"\"\"print()\n",
        "    for z, num_layers in latent_descr:\n",
        "        print(z.shape)\n",
        "        print(num_layers)\n",
        "    print()\"\"\"\n",
        "    return [(style_vectorizer(z), num_layers) for z, num_layers in latent_descr]"
      ],
      "metadata": {
        "id": "J9M3hl_-b96o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a86f930-c8e6-4a50-b4d8-d045b5a83ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_noise(n, im_size, device):\n",
        "    return torch.FloatTensor(n, im_size, im_size, 1).uniform_(0., 1.).cuda(device)"
      ],
      "metadata": {
        "id": "QbtRFf5mcAUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4b4dd9-7b6e-480f-aba4-1ae6c4f130a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(p=0.2):\n",
        "    return nn.LeakyReLU(p, inplace=True)"
      ],
      "metadata": {
        "id": "VGmeQM6ZcC88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52788703-50da-4d46-8e09-d10852a9eebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_in_chunks(max_batch_size, model, *args):\n",
        "    split_args = list(zip(*list(map(lambda x: x.split(max_batch_size, dim=0), args))))\n",
        "    chunked_outputs = [model(*i) for i in split_args]\n",
        "    if len(chunked_outputs) == 1:\n",
        "        return chunked_outputs[0]\n",
        "    return torch.cat(chunked_outputs, dim=0)"
      ],
      "metadata": {
        "id": "NTO-c8gDcErh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649ac054-99bf-4e4f-915c-f1d59d39521d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def styles_def_to_tensor(styles_def):\n",
        "    return torch.cat([t[:, None, :].expand(-1, n, -1) for t, n in styles_def], dim=1)"
      ],
      "metadata": {
        "id": "RybW7tp5cHub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7adafa0-fb82-404b-d748-62b906335477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_requires_grad(model, bool):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = bool"
      ],
      "metadata": {
        "id": "ONB3mW_DcJrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3896487e-af26-4d32-f7f7-cd0f7c50dc21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def slerp(val, low, high):\n",
        "    low_norm = low / torch.norm(low, dim=1, keepdim=True)\n",
        "    high_norm = high / torch.norm(high, dim=1, keepdim=True)\n",
        "    omega = torch.acos((low_norm * high_norm).sum(1))\n",
        "    so = torch.sin(omega)\n",
        "    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low + (torch.sin(val * omega) / so).unsqueeze(1) * high\n",
        "    return res"
      ],
      "metadata": {
        "id": "RjqDQGG3cL0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c19f108-05a8-4cae-a1d3-170b8187b017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def d_logistic_loss(fake_pred, real_pred):\n",
        "    real_loss = F.softplus(-real_pred)\n",
        "    fake_loss = F.softplus(fake_pred)\n",
        "\n",
        "    return real_loss.mean() + fake_loss.mean()"
      ],
      "metadata": {
        "id": "ZreqN586cOMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de288e71-c25e-499d-d916-8d0aca2c57f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def g_nonsaturating_loss(fake_pred):\n",
        "    loss = F.softplus(-fake_pred).mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "zgJyG7JXcRck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f837534-d525-4bea-ca95-e646dad39a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_hinge_loss(fake, real):\n",
        "    return fake.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpXettNmmPA7",
        "outputId": "f12751a5-0e3b-403d-91af-bc13354cede2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hinge_loss(fake):\n",
        "    return (F.relu(1 + real) + F.relu(1 - fake)).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHnkmwZTmQma",
        "outputId": "16a3faf5-b32d-4ff5-ed87-4f388ee47faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##StyleGan Util Class"
      ],
      "metadata": {
        "id": "LUmeuuB7mUXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EqualLinear(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, lr_mul = 1, bias = True):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_dim))\n",
        "\n",
        "        self.lr_mul = lr_mul\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self.weight * self.lr_mul, bias=self.bias * self.lr_mul)"
      ],
      "metadata": {
        "id": "r_kHRvMgmRgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5fc10e-a9ce-46d9-ddc2-a0a5052b5ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MappingNet(nn.Module):\n",
        "    def __init__(self, emb, depth, lr_mul = 0.1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i in range(depth):\n",
        "            layers.extend([EqualLinear(emb, emb, lr_mul), leaky_relu()])\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.normalize(x, dim=1)\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "VHj60aWMmevM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8ffb869-6a4a-4c52-87ed-573b17bd8964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RGBBlock(nn.Module):\n",
        "    def __init__(self, latent_dim, input_channel, upsample, rgba = False):\n",
        "        super().__init__()\n",
        "        self.input_channel = input_channel\n",
        "        self.to_style = nn.Linear(latent_dim, input_channel)\n",
        "\n",
        "        out_filters = 3 if not rgba else 4\n",
        "        self.conv = Conv2DMod(input_channel, out_filters, 1, demod=False)\n",
        "\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.Upsample(scale_factor = 2, mode='bilinear', align_corners=False),\n",
        "            Blur()\n",
        "        ) if upsample else None\n",
        "\n",
        "    def forward(self, x, prev_rgb, istyle):\n",
        "        b, c, h, w = x.shape\n",
        "        style = self.to_style(istyle)\n",
        "        x = self.conv(x, style)\n",
        "\n",
        "        if exists(prev_rgb):\n",
        "            x = x + prev_rgb\n",
        "\n",
        "        if exists(self.upsample):\n",
        "            x = self.upsample(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "A44Rd3-Smft6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c24094b-4a11-4954-8b34-b9ba8bb3be93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2DMod(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, kernel, demod=True, stride=1, dilation=1, eps = 1e-8, **kwargs):\n",
        "        super().__init__()\n",
        "        self.filters = out_chan\n",
        "        self.demod = demod\n",
        "        self.kernel = kernel\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        self.weight = nn.Parameter(torch.randn((out_chan, in_chan, kernel, kernel)))\n",
        "        self.eps = eps\n",
        "        nn.init.kaiming_normal_(self.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "    def _get_same_padding(self, size, kernel, dilation, stride):\n",
        "        return ((size - 1) * (stride - 1) + dilation * (kernel - 1)) // 2\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        w1 = y[:, None, :, None, None]\n",
        "        w2 = self.weight[None, :, :, :, :]\n",
        "        weights = w2 * (w1 + 1)\n",
        "\n",
        "        if self.demod:\n",
        "            d = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n",
        "            weights = weights * d\n",
        "\n",
        "        x = x.reshape(1, -1, h, w)\n",
        "\n",
        "        _, _, *ws = weights.shape\n",
        "        weights = weights.reshape(b * self.filters, *ws)\n",
        "\n",
        "        padding = self._get_same_padding(h, self.kernel, self.dilation, self.stride)\n",
        "        x = F.conv2d(x, weights, padding=padding, groups=b)\n",
        "\n",
        "        x = x.reshape(-1, self.filters, h, w)\n",
        "        return x"
      ],
      "metadata": {
        "id": "coNwXt3omlCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d0587e-edbb-4e80-86de-a38d07ca22c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generator"
      ],
      "metadata": {
        "id": "sOf7Dr6svZh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The GernratorBlock has all the fucntions of modulation, noise injection, convolutional operations, and RGB generation.\n",
        "It is used to generate higher-level representations of images from the input latent space.\n",
        "\"\"\"\n",
        "class GeneratorBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    __init__  Initializes linear layers (self.to_style1, self.to_noise1, self.to_style2, self.to_noise2)\n",
        "    and convolutional layers (self.conv1, self.conv2) using Conv2DMod.\n",
        "    Sets the activation function (self.activation) using leaky_relu().\n",
        "    Initializes an RGBBlock.\n",
        "\n",
        "    :self: the object of the class\n",
        "    :latent_dim: Dimensionality of the latent space\n",
        "    :input_channels: Number of input channels\n",
        "    :filters: Number of filters or channels in convolutional layers\n",
        "    :upsample: Boolean flag indicating whether to apply upsampling\n",
        "    :upsample_rgb: Boolean flag indicating whether to upsample the RGB block\n",
        "    :rgba: Boolean flag indicating whether the model uses an RGBA image representation\n",
        "    :return: describe what it returns\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim, input_channels, filters, upsample = True, upsample_rgb = True, rgba = False):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) if upsample else None # an nn.Upsample layer with bilinear upsampling settings if upsample is True\n",
        "\n",
        "        self.to_style1 = nn.Linear(latent_dim, input_channels)  #\n",
        "        self.to_noise1 = nn.Linear(1, filters)\n",
        "        self.conv1 = Conv2DMod(input_channels, filters, 3)\n",
        "\n",
        "        self.to_style2 = nn.Linear(latent_dim, filters)\n",
        "        self.to_noise2 = nn.Linear(1, filters)\n",
        "        self.conv2 = Conv2DMod(filters, filters, 3)\n",
        "\n",
        "        self.activation = leaky_relu()\n",
        "        self.to_rgb = RGBBlock(latent_dim, filters, upsample_rgb, rgba)\n",
        "\n",
        "    \"\"\"\n",
        "    :x: Input tensor to the block.\n",
        "    :prev_rgb: Previous RGB image (from the generator).\n",
        "    :istyle: Style input for the block.\n",
        "    :inoise: Noise input for the block.\n",
        "    :returns: returns an intermediate tensor x and the generated RGB output (rgb).\n",
        "    \"\"\"\n",
        "    def forward(self, x, prev_rgb, istyle, inoise):\n",
        "        if exists(self.upsample):\n",
        "            x = self.upsample(x) # Upsamples the input tensor (x) using nn.Upsample if upsample is True.\n",
        "\n",
        "        inoise = inoise[:, :x.shape[2], :x.shape[3], :] # Trims inoise to match the spatial dimensions of x\n",
        "        noise1 = self.to_noise1(inoise).permute((0, 3, 2, 1))\n",
        "        noise2 = self.to_noise2(inoise).permute((0, 3, 2, 1)) # Modifies and applies noise tensors (noise1, noise2) based on the input inoise.\n",
        "\n",
        "        style1 = self.to_style1(istyle)\n",
        "        x = self.conv1(x, style1)\n",
        "        x = self.activation(x + noise1)\n",
        "\n",
        "        style2 = self.to_style2(istyle) # Transforms style inputs (istyle) using linear layers (to_style1, to_style2)\n",
        "        x = self.conv2(x, style2) # convolutional layers (conv1, conv2) with modulation using style information.\n",
        "        x = self.activation(x + noise2) # an activation function (leaky ReLU) to the convolutional outputs\n",
        "\n",
        "        rgb = self.to_rgb(x, prev_rgb, istyle) #   RGBBlock to generate the final RGB output\n",
        "        return x, rgb"
      ],
      "metadata": {
        "id": "8DmLwPxUmoMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6583519f-b663-4d3e-a919-b54aa92fda35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It takes in latent codes, style inputs, and noise and generates RGB images by passing these inputs through\n",
        "a series of blocks that contain GeneratorBlocks and optional attention modules. The generator progressively\n",
        "upsamples the input through layers while incorporating styles and noise to generate the final RGB image output.\n",
        "\"\"\"\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "      :image_size: Size of the generated image.\n",
        "      :latent_dim: Dimensionality of the latent space.\n",
        "      :network_capacity: Capacity of the network (default is 16).\n",
        "      :transparent: Boolean flag indicating whether the model generates transparent images (default is False).\n",
        "      :attn_layers: List of layers where attention mechanism is applied (default is an empty list).\n",
        "      :fmap_max: Maximum number of feature maps (default is 512).\n",
        "    \"\"\"\n",
        "    def __init__(self, image_size, latent_dim, network_capacity = 16, transparent = False, attn_layers = [], fmap_max = 512):\n",
        "        super().__init__()\n",
        "        #Sets attributes for image_size, latent_dim, and computes the number of layers based on image_size.\n",
        "        self.image_size = image_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_layers = int(log2(image_size) - 1)\n",
        "\n",
        "        #Defines a sequence of filters for the layers based on network_capacity and image_size\n",
        "        filters = [network_capacity * (2 ** (i + 1)) for i in range(self.num_layers)][::-1]\n",
        "\n",
        "        set_fmap_max = partial(min, fmap_max)\n",
        "        filters = list(map(set_fmap_max, filters))\n",
        "        init_channels = filters[0]\n",
        "        filters = [init_channels, *filters]\n",
        "\n",
        "        in_out_pairs = zip(filters[:-1], filters[1:])\n",
        "\n",
        "        #Initializes an initial block parameter (self.initial_block) with a randomly initialized tensor.\n",
        "        self.initial_block = nn.Parameter(torch.randn((1, init_channels, 4, 4)))\n",
        "        #Sets up an initial convolutional layer (self.initial_conv)\n",
        "        self.initial_conv = nn.Conv2d(filters[0], filters[0], 3, padding=1)\n",
        "        #Initializes the blocks and attns as nn.ModuleList() for GeneratorBlock and attention modules based on the specified layers.\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        self.attns = nn.ModuleList([])\n",
        "\n",
        "        for ind, (in_chan, out_chan) in enumerate(in_out_pairs):\n",
        "            not_first = ind != 0\n",
        "            not_last = ind != (self.num_layers - 1)\n",
        "            num_layer = self.num_layers - ind\n",
        "\n",
        "            attn_fn = attn_and_ff(in_chan) if num_layer in attn_layers else None\n",
        "\n",
        "            self.attns.append(attn_fn)\n",
        "\n",
        "            block = GeneratorBlock(\n",
        "                latent_dim * 3,\n",
        "                in_chan,\n",
        "                out_chan,\n",
        "                upsample = not_first,\n",
        "                upsample_rgb = not_last,\n",
        "                rgba = transparent\n",
        "            )\n",
        "            self.blocks.append(block)\n",
        "\n",
        "    \"\"\"\n",
        "    styles_mapping: Latent codes for mapping.\n",
        "    styles_encoder: Encoded style inputs.\n",
        "    input_noise: Noise input for the generator.\n",
        "    \"\"\"\n",
        "    def forward(self, styles_mapping, styles_encoder, input_noise):\n",
        "        batch_size = styles_mapping.shape[0]\n",
        "        image_size = self.image_size\n",
        "        #Reshape and expand the styles_encoder tensor.\n",
        "        styles_encoder = styles_encoder.unsqueeze(1)\n",
        "        styles_encoder = styles_encoder.expand(\n",
        "            styles_encoder.shape[0],\n",
        "            styles_mapping.shape[1],\n",
        "            styles_encoder.shape[2]\n",
        "        )\n",
        "\n",
        "        styles = torch.cat([styles_mapping, styles_encoder], dim=2) # Expands the initial block tensor to the batch size\n",
        "\n",
        "        x = self.initial_block.expand(batch_size, -1, -1, -1) # initial convolutional layer to the expanded initial block\n",
        "\n",
        "        rgb = None\n",
        "        styles = styles.transpose(0, 1)\n",
        "        x = self.initial_conv(x)\n",
        "\n",
        "        for style, block, attn in zip(styles, self.blocks, self.attns): # Iterates through blocks, applying attention (if present) and GeneratorBlock\n",
        "            if exists(attn):\n",
        "                x = attn(x)\n",
        "            x, rgb = block(x, rgb, style, input_noise)\n",
        "\n",
        "        return rgb # Returns the generated RGB output"
      ],
      "metadata": {
        "id": "1UtvS5PVvWGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53388aca-7433-4437-f862-2e1181767842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Discriminator"
      ],
      "metadata": {
        "id": "deqm2gJTvctp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DiscriminatorBlock as a building block for a discriminator network\n",
        "It processes the input by applying convolutional operations,\n",
        "potentially downsampling the spatial dimensions,\n",
        "and adding a residual connection to the processed features,\n",
        "contributing to the hierarchical analysis of the input image for discrimination between real and generated images\n",
        "\"\"\"\n",
        "\n",
        "class DiscriminatorBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    :input_channels: Number of input channels.\n",
        "    :filters: Number of filters or channels in convolutional layers.\n",
        "    :downsample: Boolean flag indicating whether downsampling should be applied (default is True).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, filters, downsample=True):\n",
        "        super().__init__()\n",
        "        self.conv_res = nn.Conv2d(input_channels, filters, 1, stride = (2 if downsample else 1))\n",
        "        #a neural network (self.net) consisting of two convolutional layers with leaky ReLU activation functions.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, filters, 3, padding=1),\n",
        "            leaky_relu(),\n",
        "            nn.Conv2d(filters, filters, 3, padding=1),\n",
        "            leaky_relu()\n",
        "        )\n",
        "\n",
        "        self.downsample = nn.Sequential(\n",
        "            Blur(),\n",
        "            nn.Conv2d(filters, filters, 3, padding = 1, stride = 2)\n",
        "        ) if downsample else None #if downsampling is True, sets up a downsampling module (self.downsample) using a Blur layer followed by a convolutional layer with downsampling.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "     :x: Input tensor.\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        res = self.conv_res(x) # Passes the input x through the residual convolutional layer\n",
        "        x = self.net(x) #  Passes the input x through the neural network module (self.net), consisting of two convolutional layers with leaky ReLU activations.\n",
        "\n",
        "        #If downsampling is enabled, passes the result through the downsampling module (self.downsample).\n",
        "        if exists(self.downsample):\n",
        "            x = self.downsample(x)\n",
        "        #Adds the residual tensor res to the processed tensor (x) after appropriate adjustments Scales the sum by 1 / sqrt(2) (a common technique for normalization or avoiding vanishing gradients in residual connections).\n",
        "        x = (x + res) * (1 / math.sqrt(2))\n",
        "        return x"
      ],
      "metadata": {
        "id": "zX7-XK31mrkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6871f3c-dfc9-463b-cb61-c133105a3745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This Discriminator class is responsible for processing input images and masks, passing them through\n",
        "multiple blocks with optional attention and quantization, and producing logits for discrimination.\n",
        "\"\"\"\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    :image_size: Size of the input image.\n",
        "    :network_capacity: Capacity of the discriminator network (default is 16).\n",
        "    :fq_layers: List of layers for applying vector quantization (default is an empty list).\n",
        "    :fq_dict_size: Size of the dictionary for vector quantization (default is 256).\n",
        "    :attn_layers: List of layers where attention mechanism is applied (default is an empty list).\n",
        "    :transparent: Boolean flag indicating whether the model deals with transparent images (default is False).\n",
        "    :fmap_max: Maximum number of feature maps (default is 512).\n",
        "    \"\"\"\n",
        "    def __init__(self, image_size, network_capacity = 16, fq_layers = [], fq_dict_size = 256, attn_layers = [], transparent = False, fmap_max = 512):\n",
        "        super().__init__()\n",
        "        num_layers = int(log2(image_size) - 1)\n",
        "        num_init_filters = 6\n",
        "        #Determines the number of layers and initial number of filters based on the image_size.\n",
        "        blocks = []\n",
        "        filters = [num_init_filters] + [(network_capacity * 4) * (2 ** i) for i in range(num_layers + 1)]\n",
        "\n",
        "        set_fmap_max = partial(min, fmap_max)\n",
        "        filters = list(map(set_fmap_max, filters))\n",
        "        chan_in_out = list(zip(filters[:-1], filters[1:]))\n",
        "\n",
        "        blocks = []\n",
        "        attn_blocks = []\n",
        "        quantize_blocks = []\n",
        "\n",
        "        #Sets up a sequence of convolutional blocks (DiscriminatorBlock) according to the determined number of layers and filters.\n",
        "        for ind, (in_chan, out_chan) in enumerate(chan_in_out):\n",
        "            num_layer = ind + 1\n",
        "            is_not_last = ind != (len(chan_in_out) - 1)\n",
        "\n",
        "            block = DiscriminatorBlock(in_chan, out_chan, downsample = is_not_last)\n",
        "            blocks.append(block)\n",
        "\n",
        "            attn_fn = attn_and_ff(out_chan) if num_layer in attn_layers else None\n",
        "\n",
        "            attn_blocks.append(attn_fn)\n",
        "\n",
        "            quantize_fn = PermuteToFrom(VectorQuantize(out_chan, fq_dict_size)) if num_layer in fq_layers else None\n",
        "            quantize_blocks.append(quantize_fn)\n",
        "\n",
        "        #Initializes attention blocks (attn_blocks) and quantization blocks (quantize_blocks) based on the specified layers\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "        self.attn_blocks = nn.ModuleList(attn_blocks)\n",
        "        self.quantize_blocks = nn.ModuleList(quantize_blocks)\n",
        "\n",
        "        chan_last = filters[-1]\n",
        "        latent_dim = 2 * 2 * chan_last\n",
        "\n",
        "        self.final_conv = nn.Conv2d(chan_last, chan_last, 3, padding=1)\n",
        "        self.flatten = Flatten()\n",
        "        self.to_logit = nn.Linear(latent_dim, 1)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    x: Input tensor representing the image.\n",
        "    mask: Additional mask input tensor.\n",
        "    \"\"\"\n",
        "    def forward(self, x, mask):\n",
        "        b, *_ = x.shape\n",
        "\n",
        "        x = torch.cat([x, mask], dim=1) # Concatenates the input x and the mask\n",
        "\n",
        "        quantize_loss = torch.zeros(1).to(x)\n",
        "        # Iterates through the blocks, applying convolutional operations, attention (if specified), and quantization (if specified).\n",
        "        for (block, attn_block, q_block) in zip(self.blocks, self.attn_blocks, self.quantize_blocks):\n",
        "            x = block(x)\n",
        "\n",
        "            if exists(attn_block):\n",
        "                x = attn_block(x)\n",
        "            #Aggregates quantization loss from quantization blocks.\n",
        "            if exists(q_block):\n",
        "                x, _, loss = q_block(x)\n",
        "                quantize_loss += loss\n",
        "        #Performs final convolution, flattening, and a linear transformation to obtain the logits for discrimination.\n",
        "        x = self.final_conv(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.to_logit(x)\n",
        "        return x.squeeze(), quantize_loss #Returns the logits and the accumulated quantization loss"
      ],
      "metadata": {
        "id": "hp37bo6lvn5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f2f6f6e-fb56-4f1b-b9ec-077cc2766cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoder"
      ],
      "metadata": {
        "id": "iFmf2Br1KiR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "take an input image and a normalized mask, processes them through a sequence of blocks composed of DiscriminatorBlocks,\n",
        "possibly with optional attention mechanisms, and produces an encoded representation of the input,\n",
        "which could be further utilized for downstream tasks or combined with other components in a neural network architecture.\n",
        "\n",
        "\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    :image_size: Size of the input image.\n",
        "    :network_capacity: Capacity of the encoder network (default is 32).\n",
        "    :fq_layers: List of layers for applying vector quantization (default is an empty list).\n",
        "    :fq_dict_size: Size of the dictionary for vector quantization (default is 256).\n",
        "    :attn_layers: List of layers where attention mechanism is applied (default is an empty list).\n",
        "    :transparent: Boolean flag indicating whether the model deals with transparent images (default is False).\n",
        "    :fmap_max: Maximum number of feature maps (default is 512).\n",
        "    \"\"\"\n",
        "    def __init__(self, image_size, network_capacity = 32, fq_layers = [], fq_dict_size = 256, attn_layers = [], transparent = False, fmap_max = 512):\n",
        "        super().__init__()\n",
        "        #Determines the number of layers and initial number of filters based on the image_size.\n",
        "        num_layers = int(log2(image_size) - 1)\n",
        "        num_init_filters = 3 if not transparent else 4\n",
        "\n",
        "        blocks = []\n",
        "        filters = [num_init_filters + 3] + [(network_capacity * 4) * (2 ** i) for i in range(num_layers + 1)]\n",
        "\n",
        "        set_fmap_max = partial(min, fmap_max)\n",
        "        filters = list(map(set_fmap_max, filters))\n",
        "        chan_in_out = list(zip(filters[:-1], filters[1:]))\n",
        "\n",
        "        blocks = []\n",
        "        attn_blocks = []\n",
        "        #Sets up a sequence of DiscriminatorBlocks according to the determined number of layers and filters.\n",
        "        for ind, (in_chan, out_chan) in enumerate(chan_in_out):\n",
        "            num_layer = ind + 1\n",
        "            is_not_last = ind != (len(chan_in_out) - 1)\n",
        "\n",
        "            block = DiscriminatorBlock(in_chan, out_chan, downsample = is_not_last)\n",
        "            blocks.append(block)\n",
        "\n",
        "            attn_fn = attn_and_ff(out_chan) if num_layer in attn_layers else None\n",
        "\n",
        "            attn_blocks.append(attn_fn)\n",
        "\n",
        "        #Initializes attention blocks (attn_blocks) based on the specified layers.\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "        self.attn_blocks = nn.ModuleList(attn_blocks)\n",
        "\n",
        "        chan_last = filters[-1]\n",
        "        latent_dim = 2 * 2 * chan_last\n",
        "\n",
        "        self.final_conv = nn.Conv2d(chan_last, chan_last, 3, padding=1)\n",
        "        self.flatten = Flatten()\n",
        "        self.to_latent = nn.Linear(latent_dim, 1024)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    \"\"\"\n",
        "    :x: Input tensor representing the image.\n",
        "    :mask_norm_batch: Normalized mask input tensor.\n",
        "    \"\"\"\n",
        "    def forward(self, x,  mask_norm_batch):\n",
        "        b, *_ = x.shape\n",
        "\n",
        "        x = torch.cat([x, mask_norm_batch], dim=1)\n",
        "\n",
        "        for (block, attn_block) in zip(self.blocks, self.attn_blocks):\n",
        "            x = block(x)\n",
        "\n",
        "            if exists(attn_block):\n",
        "                x = attn_block(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.to_latent(x)\n",
        "        x = self.dropout(x)\n",
        "        return x.squeeze()"
      ],
      "metadata": {
        "id": "prHybzGJm8O6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06793a73-ce96-4285-f81e-b1ebf3062ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##StyleGAN2"
      ],
      "metadata": {
        "id": "u5RBZ_pLvsCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "serves as a wrapper that contains the essential components required for the StyleGAN2 architecture. It includes generators, discriminators,\n",
        "encoders, and mapping networks.\n",
        "Additionally, it implements functionalities for EMA, weight initialization, and optimizer setup, which are crucial in training\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class StyleGAN2(nn.Module):\n",
        "\n",
        "    def __init__(self, image_size, latent_dim = 512, fmap_max = 512, style_depth = 8, network_capacity = 16, transparent = False, steps = 1, lr = 1e-4, ttur_mult = 2, fq_layers = [], fq_dict_size = 256, attn_layers = [], lr_mlp = 0.1, rank = 0):\n",
        "        super().__init__()\n",
        "        self.lr = lr\n",
        "        self.steps = steps\n",
        "        self.ema_updater = EMA(0.995)\n",
        "        # Mapping network for latent space manipulation.\n",
        "        self.S = MappingNet(latent_dim, style_depth, lr_mul = lr_mlp) # Mapping network for latent space manipulation.\n",
        "        #Responsible for generating images.\n",
        "        self.G = Generator(image_size, latent_dim, network_capacity, transparent = transparent, attn_layers = attn_layers, fmap_max = fmap_max)\n",
        "        #Discriminates between real and generated images.\n",
        "        self.D = Discriminator(image_size, network_capacity, fq_layers = fq_layers, fq_dict_size = fq_dict_size, attn_layers = attn_layers, transparent = transparent, fmap_max = fmap_max)\n",
        "        #Encodes images or features.\n",
        "        self.E = Encoder(image_size, network_capacity, fq_layers = fq_layers, fq_dict_size = fq_dict_size, attn_layers = attn_layers, transparent = transparent, fmap_max = fmap_max)\n",
        "        self.SE = MappingNet(latent_dim, style_depth, lr_mul = lr_mlp)\n",
        "        self.GE = Generator(image_size, latent_dim, network_capacity, transparent = transparent, attn_layers = attn_layers)\n",
        "        self.EE = Encoder(image_size, network_capacity, fq_layers = fq_layers, fq_dict_size = fq_dict_size, attn_layers = attn_layers, transparent = transparent, fmap_max = fmap_max)\n",
        "\n",
        "        print(self.S)\n",
        "        print(self.D)\n",
        "        print(self.G)\n",
        "        print(self.E)\n",
        "\n",
        "\n",
        "        self.D_cl = None\n",
        "\n",
        "        # turn off grad for exponential moving averages\n",
        "        set_requires_grad(self.SE, False)\n",
        "        set_requires_grad(self.GE, False)\n",
        "        set_requires_grad(self.EE, False)\n",
        "\n",
        "        # init optimizers\n",
        "        generator_params = list(self.G.parameters()) + list(self.S.parameters()) + list(self.E.parameters())\n",
        "        self.G_opt = Adam(generator_params, lr = self.lr, betas=(0.5, 0.9))\n",
        "        self.D_opt = Adam(self.D.parameters(), lr = self.lr * ttur_mult, betas=(0.5, 0.9))\n",
        "\n",
        "\n",
        "        # init weights\n",
        "        self._init_weights()\n",
        "        self.reset_parameter_averaging()\n",
        "\n",
        "        self.cuda(rank)\n",
        "\n",
        "    \"\"\"\n",
        "    Initializes weights of convolutional and linear layers using Kaiming normal initialization,\n",
        "    specifically focusing on layers of type nn.Conv2d and nn.Linear.\n",
        "    Zeros the weights and biases of noise layers within the generator blocks.\n",
        "    \"\"\"\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if type(m) in {nn.Conv2d, nn.Linear}:\n",
        "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "        for block in self.G.blocks:\n",
        "            nn.init.zeros_(block.to_noise1.weight)\n",
        "            nn.init.zeros_(block.to_noise2.weight)\n",
        "            nn.init.zeros_(block.to_noise1.bias)\n",
        "            nn.init.zeros_(block.to_noise2.bias)\n",
        "\n",
        "    \"\"\"\n",
        "    Implements exponential moving average (EMA) update for maintaining the moving averages\n",
        "    of specific model parameters. Updates the EMA models using an EMA updater.\n",
        "    \"\"\"\n",
        "    def EMA(self):\n",
        "        def update_moving_average(ma_model, current_model):\n",
        "            for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "                old_weight, up_weight = ma_params.data, current_params.data\n",
        "                ma_params.data = self.ema_updater.update_average(old_weight, up_weight)\n",
        "\n",
        "        update_moving_average(self.SE, self.S)\n",
        "        update_moving_average(self.GE, self.G)\n",
        "        update_moving_average(self.EE, self.E)\n",
        "\n",
        "    \"\"\"\n",
        "    Resets parameter averaging by loading the state dictionary of the main models into the models used for EMA.\n",
        "    Placeholder method that returns the input x as it is.\n",
        "    \"\"\"\n",
        "    def reset_parameter_averaging(self):\n",
        "        self.SE.load_state_dict(self.S.state_dict())\n",
        "        self.GE.load_state_dict(self.G.state_dict())\n",
        "        self.EE.load_state_dict(self.E.state_dict())\n",
        "\n",
        "    \"\"\"\n",
        "    Placeholder method that returns the input x as it is.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjZ2QdovnZE5",
        "outputId": "b32455ef-09ba-4dc4-ad93-d33b7b09403d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_image(real_images, fake_images, mask):\n",
        "    return (mask * fake_images + (1 - mask) * real_images).cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io8DuFhdnazi",
        "outputId": "e1da7ade-e328-4e89-9e6d-d43000f1a039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "mQgUwambvybi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    def __init__(\n",
        "        self,\n",
        "        name = 'default',\n",
        "        results_dir = 'results',\n",
        "        models_dir = 'models',\n",
        "        base_dir = './',\n",
        "        image_size = 128,\n",
        "        network_capacity = 32,\n",
        "        fmap_max = 512,\n",
        "        transparent = False,\n",
        "        batch_size = 4,\n",
        "        mixed_prob = 0.9,\n",
        "        lr = 2e-4,\n",
        "        lr_mlp = 0.1,\n",
        "        ttur_mult = 2,\n",
        "        num_workers = None,\n",
        "        save_every = 1000,\n",
        "        evaluate_every = 1000,\n",
        "        num_image_tiles = 8,\n",
        "        trunc_psi = 0.6,\n",
        "        no_pl_reg = False,\n",
        "        fq_layers = [],\n",
        "        fq_dict_size = 256,\n",
        "        attn_layers = [],\n",
        "        aug_types = ['translation', 'cutout'],\n",
        "        generator_top_k_gamma = 0.99,\n",
        "        generator_top_k_frac = 0.5,\n",
        "        calculate_fid_every = None,\n",
        "        calculate_fid_num_images = 12800,\n",
        "        clear_fid_cache = False,\n",
        "        rank = 0,\n",
        "        world_size = 1,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        self.GAN_params = [args, kwargs]\n",
        "        self.GAN = None\n",
        "\n",
        "        self.name = name\n",
        "\n",
        "        base_dir = Path(base_dir)\n",
        "        self.base_dir = base_dir\n",
        "        self.results_dir = base_dir / results_dir\n",
        "        self.models_dir = base_dir / models_dir\n",
        "        self.fid_dir = base_dir / 'fid' / name\n",
        "        self.config_path = self.models_dir / name / '.config.json'\n",
        "\n",
        "        assert log2(image_size).is_integer(), 'image size must be a power of 2 (64, 128, 256, 512, 1024)'\n",
        "        self.image_size = image_size\n",
        "        self.network_capacity = network_capacity\n",
        "        self.fmap_max = fmap_max\n",
        "        self.transparent = transparent\n",
        "\n",
        "        self.fq_layers = cast_list(fq_layers)\n",
        "        self.fq_dict_size = fq_dict_size\n",
        "        self.has_fq = len(self.fq_layers) > 0\n",
        "\n",
        "        self.attn_layers = cast_list(attn_layers)\n",
        "\n",
        "        self.aug_types = aug_types\n",
        "\n",
        "        self.lr = lr\n",
        "        self.lr_mlp = lr_mlp\n",
        "        self.ttur_mult = ttur_mult\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.mixed_prob = mixed_prob\n",
        "\n",
        "        self.num_image_tiles = num_image_tiles\n",
        "        self.evaluate_every = evaluate_every\n",
        "        self.save_every = save_every\n",
        "        self.steps = 0\n",
        "\n",
        "        self.av = None\n",
        "        self.trunc_psi = trunc_psi\n",
        "\n",
        "        self.no_pl_reg = no_pl_reg\n",
        "        self.pl_mean = None\n",
        "\n",
        "        self.d_loss = 0\n",
        "        self.d_loss_ema = 0\n",
        "        self.g_loss = 0\n",
        "        self.g_loss_ema = 0\n",
        "        self.q_loss = None\n",
        "        self.last_gp_loss = None\n",
        "        self.last_cr_loss = None\n",
        "        self.last_fid = None\n",
        "\n",
        "        self.pl_length_ma = EMA(0.99)\n",
        "        self.init_folders()\n",
        "\n",
        "        self.loader = None\n",
        "\n",
        "        self.calculate_fid_every = calculate_fid_every\n",
        "        self.calculate_fid_num_images = calculate_fid_num_images\n",
        "        self.clear_fid_cache = clear_fid_cache\n",
        "\n",
        "        self.generator_top_k_gamma = generator_top_k_gamma\n",
        "        self.generator_top_k_frac = generator_top_k_frac\n",
        "\n",
        "        self.is_main = rank == 0\n",
        "        self.rank = rank\n",
        "        self.world_size = world_size\n",
        "\n",
        "    @property\n",
        "    def image_extension(self):\n",
        "        return 'jpg' if not self.transparent else 'png'\n",
        "\n",
        "    @property\n",
        "    def checkpoint_num(self):\n",
        "        return floor(self.steps // self.save_every)\n",
        "\n",
        "    @property\n",
        "    def hparams(self):\n",
        "        return {'image_size': self.image_size, 'network_capacity': self.network_capacity}\n",
        "\n",
        "    def init_GAN(self):\n",
        "        args, kwargs = self.GAN_params\n",
        "        self.GAN = StyleGAN2(lr = self.lr, lr_mlp = self.lr_mlp, ttur_mult = self.ttur_mult, image_size = self.image_size, network_capacity = self.network_capacity, fmap_max = self.fmap_max, transparent = self.transparent, fq_layers = self.fq_layers, fq_dict_size = self.fq_dict_size, attn_layers = self.attn_layers, rank = self.rank, *args, **kwargs)\n",
        "\n",
        "\n",
        "    def write_config(self):\n",
        "        self.config_path.write_text(json.dumps(self.config()))\n",
        "\n",
        "    def load_config(self):\n",
        "        config = self.config() if not self.config_path.exists() else json.loads(self.config_path.read_text())\n",
        "        self.image_size = config['image_size']\n",
        "        self.network_capacity = config['network_capacity']\n",
        "        self.transparent = config['transparent']\n",
        "        self.fq_layers = config['fq_layers']\n",
        "        self.fq_dict_size = config['fq_dict_size']\n",
        "        self.fmap_max = config.pop('fmap_max', 512)\n",
        "        self.attn_layers = config.pop('attn_layers', [])\n",
        "        self.lr_mlp = config.pop('lr_mlp', 0.1)\n",
        "        del self.GAN\n",
        "        self.init_GAN()\n",
        "\n",
        "    def config(self):\n",
        "        return {'image_size': self.image_size, 'network_capacity': self.network_capacity, 'lr_mlp': self.lr_mlp, 'transparent': self.transparent, 'fq_layers': self.fq_layers, 'fq_dict_size': self.fq_dict_size, 'attn_layers': self.attn_layers}\n",
        "\n",
        "    def set_data_src(self, folder):\n",
        "        self.dataset = ImageDataset(image_paths)\n",
        "        num_workers = NUM_CORES\n",
        "        dataloader = data.DataLoader(\n",
        "            self.dataset,\n",
        "            num_workers = num_workers,\n",
        "            batch_size = self.batch_size,\n",
        "            shuffle = True,\n",
        "            drop_last = True,\n",
        "            pin_memory = True\n",
        "        )\n",
        "        self.loader = cycle(dataloader)\n",
        "\n",
        "    def train(self):\n",
        "        assert exists(self.loader), 'You must first initialize the data source with `.set_data_src(<folder of images>)`'\n",
        "\n",
        "        if not exists(self.GAN):\n",
        "            self.init_GAN()\n",
        "\n",
        "        self.GAN.train()\n",
        "        total_disc_loss = torch.tensor(0.).cuda(self.rank)\n",
        "        total_gen_loss = torch.tensor(0.).cuda(self.rank)\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "\n",
        "        image_size = self.GAN.G.image_size\n",
        "        latent_dim = self.GAN.G.latent_dim\n",
        "        num_layers = self.GAN.G.num_layers\n",
        "\n",
        "        aug_types  = self.aug_types\n",
        "        aug_kwargs = {'types': aug_types}\n",
        "\n",
        "        apply_gradient_penalty = self.steps % 4 == 0\n",
        "        apply_path_penalty = not self.no_pl_reg and self.steps > 5000 and self.steps % 32 == 0\n",
        "\n",
        "        S = self.GAN.S\n",
        "        G = self.GAN.G\n",
        "        D = self.GAN.D\n",
        "        E = self.GAN.E\n",
        "\n",
        "        # setup losses\n",
        "        D_loss_fn = d_logistic_loss\n",
        "        G_loss_fn = g_nonsaturating_loss\n",
        "        G_requires_reals = False\n",
        "\n",
        "\n",
        "        # train discriminator\n",
        "        avg_pl_length = self.pl_mean\n",
        "        self.GAN.D_opt.zero_grad()\n",
        "\n",
        "        get_latents_fn = mixed_list if random() < self.mixed_prob else noise_list\n",
        "        style = get_latents_fn(batch_size, num_layers, latent_dim, device=self.rank)\n",
        "        noise = image_noise(batch_size, image_size, device=self.rank)\n",
        "\n",
        "        w_space = latent_to_w(S, style)\n",
        "        w_styles_mapping = styles_def_to_tensor(w_space)\n",
        "\n",
        "        image_batch, image_cut_batch, mask_norm_batch, mask_batch = next(self.loader)\n",
        "        image_batch, image_cut_batch, mask_norm_batch, mask_batch = image_batch.cuda(), image_cut_batch.cuda(), mask_norm_batch.cuda(), mask_batch.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            w_styles_encoder = E(image_cut_batch, mask_norm_batch)\n",
        "\n",
        "        generated_images = G(w_styles_mapping, w_styles_encoder, noise)\n",
        "        fake_output, fake_q_loss = D(reconstruct_image(image_batch, generated_images.clone().detach(), mask_batch), mask_norm_batch)\n",
        "\n",
        "        image_batch.requires_grad_()\n",
        "        real_output, real_q_loss = D(image_batch, mask_norm_batch)\n",
        "\n",
        "\n",
        "        divergence = D_loss_fn(fake_output, real_output)\n",
        "        disc_loss = divergence\n",
        "\n",
        "        if self.has_fq:\n",
        "            quantize_loss = (fake_q_loss + real_q_loss).mean()\n",
        "            self.q_loss = float(quantize_loss.detach().item())\n",
        "\n",
        "            disc_loss = disc_loss + quantize_loss\n",
        "\n",
        "        if apply_gradient_penalty:\n",
        "            gp = gradient_penalty(image_batch, real_output)\n",
        "            self.last_gp_loss = gp.clone().detach().item()\n",
        "            self.track(self.last_gp_loss, 'GP')\n",
        "            disc_loss = disc_loss + gp\n",
        "\n",
        "        disc_loss.register_hook(raise_if_nan)\n",
        "        disc_loss.backward()\n",
        "\n",
        "        total_disc_loss += divergence.detach().item()\n",
        "\n",
        "        self.d_loss = float(total_disc_loss)\n",
        "        self.d_loss_ema = 0.98  * self.d_loss_ema + 0.02 * self.d_loss\n",
        "\n",
        "        self.track(self.d_loss, 'D')\n",
        "\n",
        "        self.GAN.D_opt.step()\n",
        "\n",
        "        # train generator\n",
        "        nn.utils.clip_grad_norm_(D.parameters(), 1.0)\n",
        "        self.GAN.G_opt.zero_grad()\n",
        "\n",
        "        style = get_latents_fn(batch_size, num_layers, latent_dim, device=self.rank)\n",
        "        noise = image_noise(batch_size, image_size, device=self.rank)\n",
        "\n",
        "        w_space = latent_to_w(S, style)\n",
        "        w_styles_mapping = styles_def_to_tensor(w_space)\n",
        "\n",
        "        w_styles_encoder = E(image_cut_batch, mask_norm_batch)\n",
        "\n",
        "        generated_images = G(w_styles_mapping, w_styles_encoder, noise)\n",
        "        fake_output, _ = D(reconstruct_image(image_batch, generated_images, mask_batch), mask_norm_batch)\n",
        "\n",
        "        real_output = None\n",
        "        if G_requires_reals:\n",
        "            image_batch = next(self.loader).cuda(self.rank)\n",
        "            real_output, _ = D(image_batch.detach())\n",
        "            real_output = real_output.detach()\n",
        "\n",
        "        loss = G_loss_fn(fake_output)\n",
        "        gen_loss = loss\n",
        "\n",
        "        if apply_path_penalty:\n",
        "            pl_lengths = calc_pl_lengths(w_styles_mapping, generated_images)\n",
        "            avg_pl_length = np.mean(pl_lengths.detach().cpu().numpy())\n",
        "\n",
        "            if not is_empty(self.pl_mean):\n",
        "                pl_loss = ((pl_lengths - self.pl_mean) ** 2).mean()\n",
        "                if not torch.isnan(pl_loss):\n",
        "                    gen_loss = gen_loss + pl_loss\n",
        "\n",
        "        gen_loss.register_hook(raise_if_nan)\n",
        "        gen_loss.backward()\n",
        "\n",
        "        total_gen_loss += loss.detach().item()\n",
        "\n",
        "        self.g_loss = float(total_gen_loss)\n",
        "        self.g_loss_ema = 0.98  * self.g_loss_ema + 0.02 * self.g_loss\n",
        "        self.track(self.g_loss, 'G')\n",
        "\n",
        "        nn.utils.clip_grad_norm_(list(G.parameters()) + list(S.parameters()) + list(E.parameters()), 1.0)\n",
        "        self.GAN.G_opt.step()\n",
        "\n",
        "        # calculate moving averages\n",
        "\n",
        "        if apply_path_penalty and not np.isnan(avg_pl_length):\n",
        "            self.pl_mean = self.pl_length_ma.update_average(self.pl_mean, avg_pl_length)\n",
        "            self.track(self.pl_mean, 'PL')\n",
        "\n",
        "        if self.is_main and self.steps % 10 == 0 and self.steps > 20000:\n",
        "            self.GAN.EMA()\n",
        "\n",
        "        if self.is_main and self.steps <= 25000 and self.steps % 1000 == 2:\n",
        "            self.GAN.reset_parameter_averaging()\n",
        "\n",
        "        # save from NaN errors\n",
        "\n",
        "        if any(torch.isnan(l) for l in (total_gen_loss, total_disc_loss)):\n",
        "            print(f'NaN detected for generator or discriminator. Loading from checkpoint #{self.checkpoint_num}')\n",
        "            self.load(self.checkpoint_num)\n",
        "            raise NanException\n",
        "\n",
        "        if self.steps % 10 == 0:\n",
        "            gc.collect()\n",
        "\n",
        "        # periodically save results\n",
        "        if self.steps % 100 == 0 and self.steps != 0:\n",
        "            print(f\"gen_loss: {round(self.g_loss_ema, 4)} disc_loss: {round(self.d_loss_ema, 4)}\")\n",
        "            fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n",
        "            ax[0, 0].imshow((image_cut_batch[0].cpu().detach().permute(1, 2, 0).numpy() * 0.5) + 0.5)\n",
        "            ax[0, 1].imshow((generated_images[0].cpu().detach().permute(1, 2, 0).numpy() * 0.5) + 0.5)\n",
        "            ax[1, 0].imshow(mask_batch[0].cpu().permute(1, 2, 0).numpy())\n",
        "            ax[1, 1].imshow((reconstruct_image(image_batch, generated_images.clone().detach(), mask_batch)[0].detach().cpu().permute(1, 2, 0).numpy() * 0.5) + 0.5)\n",
        "            plt.show()\n",
        "\n",
        "        if self.steps % self.save_every == 0:\n",
        "            self.save(self.checkpoint_num)\n",
        "\n",
        "        self.steps += 1\n",
        "        self.av = None\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, num = 0, trunc = 1.0):\n",
        "        self.GAN.eval()\n",
        "        ext = self.image_extension\n",
        "        num_rows = self.num_image_tiles\n",
        "\n",
        "        latent_dim = self.GAN.G.latent_dim\n",
        "        image_size = self.GAN.G.image_size\n",
        "        num_layers = self.GAN.G.num_layers\n",
        "\n",
        "        # latents and noise\n",
        "\n",
        "        latents = noise_list(num_rows ** 2, num_layers, latent_dim, device=self.rank)\n",
        "        n = image_noise(num_rows ** 2, image_size, device=self.rank)\n",
        "\n",
        "        # regular\n",
        "\n",
        "        generated_images = self.generate_truncated(self.GAN.S, self.GAN.G, latents, n, trunc_psi = self.trunc_psi)\n",
        "        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}.{ext}'), nrow=num_rows)\n",
        "\n",
        "        # moving averages\n",
        "\n",
        "        generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, latents, n, trunc_psi = self.trunc_psi)\n",
        "        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}-ema.{ext}'), nrow=num_rows)\n",
        "\n",
        "        # mixing regularities\n",
        "\n",
        "        def tile(a, dim, n_tile):\n",
        "            init_dim = a.size(dim)\n",
        "            repeat_idx = [1] * a.dim()\n",
        "            repeat_idx[dim] = n_tile\n",
        "            a = a.repeat(*(repeat_idx))\n",
        "            order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).cuda(self.rank)\n",
        "            return torch.index_select(a, dim, order_index)\n",
        "\n",
        "        nn = noise(num_rows, latent_dim, device=self.rank)\n",
        "        tmp1 = tile(nn, 0, num_rows)\n",
        "        tmp2 = nn.repeat(num_rows, 1)\n",
        "\n",
        "        tt = int(num_layers / 2)\n",
        "        mixed_latents = [(tmp1, tt), (tmp2, num_layers - tt)]\n",
        "\n",
        "        generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, mixed_latents, n, trunc_psi = self.trunc_psi)\n",
        "        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}-mr.{ext}'), nrow=num_rows)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def calculate_fid(self, num_batches):\n",
        "        from pytorch_fid import fid_score\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        real_path = self.fid_dir / 'real'\n",
        "        fake_path = self.fid_dir / 'fake'\n",
        "\n",
        "        # remove any existing files used for fid calculation and recreate directories\n",
        "\n",
        "        if not real_path.exists() or self.clear_fid_cache:\n",
        "            rmtree(real_path, ignore_errors=True)\n",
        "            os.makedirs(real_path)\n",
        "\n",
        "            for batch_num in tqdm(range(num_batches), desc='calculating FID - saving reals'):\n",
        "                real_batch = next(self.loader)\n",
        "                for k, image in enumerate(real_batch.unbind(0)):\n",
        "                    filename = str(k + batch_num * self.batch_size)\n",
        "                    torchvision.utils.save_image(image, str(real_path / f'{filename}.png'))\n",
        "\n",
        "        # generate a bunch of fake images in results / name / fid_fake\n",
        "\n",
        "        rmtree(fake_path, ignore_errors=True)\n",
        "        os.makedirs(fake_path)\n",
        "\n",
        "        self.GAN.eval()\n",
        "        ext = self.image_extension\n",
        "\n",
        "        latent_dim = self.GAN.G.latent_dim\n",
        "        image_size = self.GAN.G.image_size\n",
        "        num_layers = self.GAN.G.num_layers\n",
        "\n",
        "        for batch_num in tqdm(range(num_batches), desc='calculating FID - saving generated'):\n",
        "            # latents and noise\n",
        "            latents = noise_list(self.batch_size, num_layers, latent_dim, device=self.rank)\n",
        "            noise = image_noise(self.batch_size, image_size, device=self.rank)\n",
        "\n",
        "            # moving averages\n",
        "            generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, latents, noise, trunc_psi = self.trunc_psi)\n",
        "\n",
        "            for j, image in enumerate(generated_images.unbind(0)):\n",
        "                torchvision.utils.save_image(image, str(fake_path / f'{str(j + batch_num * self.batch_size)}-ema.{ext}'))\n",
        "\n",
        "        return fid_score.calculate_fid_given_paths([str(real_path), str(fake_path)], 256, noise.device, 2048)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def truncate_style(self, tensor, trunc_psi = 0.75):\n",
        "        S = self.GAN.S\n",
        "        batch_size = self.batch_size\n",
        "        latent_dim = self.GAN.G.latent_dim\n",
        "\n",
        "        if not exists(self.av):\n",
        "            z = noise(2000, latent_dim, device=self.rank)\n",
        "            samples = evaluate_in_chunks(batch_size, S, z).cpu().numpy()\n",
        "            self.av = np.mean(samples, axis = 0)\n",
        "            self.av = np.expand_dims(self.av, axis = 0)\n",
        "\n",
        "        av_torch = torch.from_numpy(self.av).cuda(self.rank)\n",
        "        tensor = trunc_psi * (tensor - av_torch) + av_torch\n",
        "        return tensor\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def truncate_style_defs(self, w, trunc_psi = 0.75):\n",
        "        w_space = []\n",
        "        for tensor, num_layers in w:\n",
        "            tensor = self.truncate_style(tensor, trunc_psi = trunc_psi)\n",
        "            w_space.append((tensor, num_layers))\n",
        "        return w_space\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_truncated(self, S, G, style, noi, trunc_psi = 0.75, num_image_tiles = 8):\n",
        "        w = map(lambda t: (S(t[0]), t[1]), style)\n",
        "        w_truncated = self.truncate_style_defs(w, trunc_psi = trunc_psi)\n",
        "        w_styles = styles_def_to_tensor(w_truncated)\n",
        "        generated_images = evaluate_in_chunks(self.batch_size, G, w_styles, noi)\n",
        "        return generated_images.clamp_(0., 1.)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_interpolation(self, num = 0, num_image_tiles = 8, trunc = 1.0, num_steps = 100, save_frames = False):\n",
        "        self.GAN.eval()\n",
        "        ext = self.image_extension\n",
        "        num_rows = num_image_tiles\n",
        "\n",
        "        latent_dim = self.GAN.G.latent_dim\n",
        "        image_size = self.GAN.G.image_size\n",
        "        num_layers = self.GAN.G.num_layers\n",
        "\n",
        "        # latents and noise\n",
        "\n",
        "        latents_low = noise(num_rows ** 2, latent_dim, device=self.rank)\n",
        "        latents_high = noise(num_rows ** 2, latent_dim, device=self.rank)\n",
        "        n = image_noise(num_rows ** 2, image_size, device=self.rank)\n",
        "\n",
        "        ratios = torch.linspace(0., 8., num_steps)\n",
        "\n",
        "        frames = []\n",
        "        for ratio in tqdm(ratios):\n",
        "            interp_latents = slerp(ratio, latents_low, latents_high)\n",
        "            latents = [(interp_latents, num_layers)]\n",
        "            generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, latents, n, trunc_psi = self.trunc_psi)\n",
        "            images_grid = torchvision.utils.make_grid(generated_images, nrow = num_rows)\n",
        "            pil_image = transforms.ToPILImage()(images_grid.cpu())\n",
        "\n",
        "            if self.transparent:\n",
        "                background = Image.new(\"RGBA\", pil_image.size, (255, 255, 255))\n",
        "                pil_image = Image.alpha_composite(background, pil_image)\n",
        "\n",
        "            frames.append(pil_image)\n",
        "\n",
        "        frames[0].save(str(self.results_dir / self.name / f'{str(num)}.gif'), save_all=True, append_images=frames[1:], duration=80, loop=0, optimize=True)\n",
        "\n",
        "        if save_frames:\n",
        "            folder_path = (self.results_dir / self.name / f'{str(num)}')\n",
        "            folder_path.mkdir(parents=True, exist_ok=True)\n",
        "            for ind, frame in enumerate(frames):\n",
        "                frame.save(str(folder_path / f'{str(ind)}.{ext}'))\n",
        "\n",
        "    def print_log(self):\n",
        "        data = [\n",
        "            ('G', self.g_loss),\n",
        "            ('D', self.d_loss),\n",
        "            ('GP', self.last_gp_loss),\n",
        "            ('PL', self.pl_mean),\n",
        "            ('CR', self.last_cr_loss),\n",
        "            ('Q', self.q_loss),\n",
        "            ('FID', self.last_fid)\n",
        "        ]\n",
        "\n",
        "        data = [d for d in data if exists(d[1])]\n",
        "        log = ' | '.join(map(lambda n: f'{n[0]}: {n[1]:.2f}', data))\n",
        "        print(log)\n",
        "\n",
        "    def track(self, value, name):\n",
        "        pass\n",
        "\n",
        "    def model_name(self, num):\n",
        "        return str(self.models_dir / self.name / f'model_{num}.pt')\n",
        "\n",
        "    def init_folders(self):\n",
        "        (self.results_dir / self.name).mkdir(parents=True, exist_ok=True)\n",
        "        (self.models_dir / self.name).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def clear(self):\n",
        "        rmtree(str(self.models_dir / self.name), True)\n",
        "        rmtree(str(self.results_dir / self.name), True)\n",
        "        rmtree(str(self.fid_dir), True)\n",
        "        rmtree(str(self.config_path), True)\n",
        "        self.init_folders()\n",
        "\n",
        "    def save(self, num):\n",
        "        save_data = {\n",
        "            'GAN': self.GAN.state_dict(),\n",
        "        }\n",
        "\n",
        "        torch.save(save_data, self.model_name(num))\n",
        "        self.write_config()\n",
        "\n",
        "    def load(self, num = -1):\n",
        "        self.load_config()\n",
        "\n",
        "        load_data = torch.load(\"../input/stylegan/models/default/model_6.pt\")\n",
        "        if 'version' in load_data:\n",
        "            print(f\"loading from version {load_data['version']}\")\n",
        "\n",
        "        try:\n",
        "            self.GAN.load_state_dict(load_data['GAN'])\n",
        "        except Exception as e:\n",
        "            print('unable to load save model. please try downgrading the package to the version specified by the saved model')\n",
        "            raise e"
      ],
      "metadata": {
        "id": "NFcqg_5Cn59l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d0339f-0a5a-45cd-8c53-924bf2db7627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelLoader:\n",
        "    def __init__(self, *, base_dir, name = 'default', load_from = -1):\n",
        "        self.model = Trainer(name = name, base_dir = base_dir)\n",
        "        self.model.load(load_from)\n",
        "\n",
        "    def noise_to_styles(self, noise, trunc_psi = None):\n",
        "        noise = noise.cuda()\n",
        "        w = self.model.GAN.SE(noise)\n",
        "        if exists(trunc_psi):\n",
        "            w = self.model.truncate_style(w)\n",
        "        return w\n",
        "\n",
        "    def styles_to_images(self, w):\n",
        "        batch_size, *_ = w.shape\n",
        "        num_layers = self.model.GAN.GE.num_layers\n",
        "        image_size = self.model.image_size\n",
        "        w_def = [(w, num_layers)]\n",
        "\n",
        "        w_tensors = styles_def_to_tensor(w_def)\n",
        "        noise = image_noise(batch_size, image_size, device = 0)\n",
        "\n",
        "        images = self.model.GAN.GE(w_tensors, noise)\n",
        "        images.clamp_(0., 1.)\n",
        "        return images"
      ],
      "metadata": {
        "id": "gWKQBeXk-lpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0656a3ee-91f2-4346-cce0-c006979b933e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cast_list(el):\n",
        "    return el if isinstance(el, list) else [el]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vons3n1ors3",
        "outputId": "ba30ab72-d017-4dc1-8eff-89759ffe828b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def timestamped_filename(prefix = 'generated-'):\n",
        "    now = datetime.now()\n",
        "    timestamp = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
        "    return f'{prefix}{timestamp}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hygj3-NuouAR",
        "outputId": "335df530-f263-4894-f5c1-b526dffc6c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "GaTeEU0oowXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7c9bb9-9d5e-406d-d822-a640885b2b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(rank, world_size, model_args, data, load_from, new, num_train_steps, name, seed):\n",
        "    is_main = rank == 0\n",
        "\n",
        "    model_args.update(\n",
        "        rank = rank,\n",
        "        world_size = world_size\n",
        "    )\n",
        "\n",
        "    model = Trainer(**model_args)\n",
        "\n",
        "    if not new:\n",
        "        model.load(load_from)\n",
        "    else:\n",
        "        model.clear()\n",
        "\n",
        "    model.set_data_src(data)\n",
        "\n",
        "    for _ in tqdm(range(num_train_steps - model.steps), initial = model.steps, total = num_train_steps):\n",
        "        model.train()\n",
        "\n",
        "    model.save(model.checkpoint_num)"
      ],
      "metadata": {
        "id": "-_IkRd-No0rB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b957630-2729-47e5-f39d-03063cbbdade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_from_folder(\n",
        "    data = '../input/flickrfaceshq-dataset-ffhq',\n",
        "    results_dir = './results',\n",
        "    models_dir = './models',\n",
        "    name = 'default',\n",
        "    new =  True,\n",
        "    load_from = -1,\n",
        "    image_size = 128,\n",
        "    network_capacity = 16,\n",
        "    fmap_max = 512,\n",
        "    transparent = False,\n",
        "    batch_size = 14,\n",
        "    num_train_steps = 16000,\n",
        "    learning_rate = 2e-4,\n",
        "    lr_mlp = 0.1,\n",
        "    ttur_mult = 1.5,\n",
        "    num_workers =  None,\n",
        "    save_every = 1000,\n",
        "    evaluate_every = 1000,\n",
        "    generate = False,\n",
        "    num_generate = 1,\n",
        "    generate_interpolation = False,\n",
        "    interpolation_num_steps = 100,\n",
        "    save_frames = False,\n",
        "    num_image_tiles = 8,\n",
        "    trunc_psi = 0.75,\n",
        "    mixed_prob = 0.9,\n",
        "    no_pl_reg = False,\n",
        "    fq_layers = [],\n",
        "    fq_dict_size = 256,\n",
        "    attn_layers = [],\n",
        "    aug_types = ['translation', 'cutout'],\n",
        "    generator_top_k_gamma = 0.99,\n",
        "    generator_top_k_frac = 0.5,\n",
        "    calculate_fid_every = None,\n",
        "    calculate_fid_num_images = 12800,\n",
        "    clear_fid_cache = False,\n",
        "    seed = 42,\n",
        "):\n",
        "    model_args = dict(\n",
        "        name = name,\n",
        "        results_dir = results_dir,\n",
        "        models_dir = models_dir,\n",
        "        batch_size = batch_size,\n",
        "        image_size = image_size,\n",
        "        network_capacity = network_capacity,\n",
        "        fmap_max = fmap_max,\n",
        "        transparent = transparent,\n",
        "        lr = learning_rate,\n",
        "        lr_mlp = lr_mlp,\n",
        "        ttur_mult = ttur_mult,\n",
        "        num_workers = num_workers,\n",
        "        save_every = save_every,\n",
        "        evaluate_every = evaluate_every,\n",
        "        num_image_tiles = num_image_tiles,\n",
        "        trunc_psi = trunc_psi,\n",
        "        no_pl_reg = no_pl_reg,\n",
        "        fq_layers = fq_layers,\n",
        "        fq_dict_size = fq_dict_size,\n",
        "        attn_layers = attn_layers,\n",
        "        aug_types = cast_list(aug_types),\n",
        "        generator_top_k_gamma = generator_top_k_gamma,\n",
        "        generator_top_k_frac = generator_top_k_frac,\n",
        "        calculate_fid_every = calculate_fid_every,\n",
        "        calculate_fid_num_images = calculate_fid_num_images,\n",
        "        clear_fid_cache = clear_fid_cache,\n",
        "        mixed_prob = mixed_prob,\n",
        "    )\n",
        "\n",
        "    if generate:\n",
        "        model = Trainer(**model_args)\n",
        "        model.load(load_from)\n",
        "        samples_name = timestamped_filename()\n",
        "        for num in tqdm(range(num_generate)):\n",
        "            model.evaluate(f'{samples_name}-{num}', num_image_tiles)\n",
        "\n",
        "        print(f'sample images generated at {results_dir}/{name}/{samples_name}')\n",
        "        return\n",
        "\n",
        "    if generate_interpolation:\n",
        "        model = Trainer(**model_args)\n",
        "        model.load(load_from)\n",
        "        samples_name = timestamped_filename()\n",
        "        model.generate_interpolation(samples_name, num_image_tiles, num_steps = interpolation_num_steps, save_frames = save_frames)\n",
        "        print(f'interpolation generated at {results_dir}/{name}/{samples_name}')\n",
        "        return\n",
        "\n",
        "\n",
        "    run_training(0, 1, model_args, data, load_from, new, num_train_steps, name, seed)"
      ],
      "metadata": {
        "id": "nr5e7rlf-x36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece70343-de35-4b8b-f0a7-b306c564e6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_from_folder()"
      ],
      "metadata": {
        "id": "pZZlylHNnKQS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}